## join ids and predictions
dat_pred_test_cnn_lstm <- bind_cols(y_test, ypred_test_cnn_lstm) %>%
rename(obs = `score`, pred = `...2`) %>%
mutate(obs = as.factor(ifelse(obs > 0, 1,0)),
pred = as.factor(ifelse(pred > 0, 1, 0)))
ConfusionMat <- dat_pred_test_cnn_lstm %>% conf_mat(obs, pred)
# autoplot(ConfusionMat, type = "mosaic")
# autoplot(ConfusionMat, type = "heatmap")
custom_metrics <- metric_set(yardstick::accuracy, sens, spec, ppv, kap, mcc)
metrics_test_cnn_lstm <- custom_metrics(dat_pred_test_cnn_lstm,
truth = obs,
estimate = pred)
print(metrics_test_cnn_lstm)
model_cnn_lstm <- keras_model_sequential() %>%
# embedding layer
layer_embedding(name = "input",
input_dim = vocab_size + 1,
output_dim = emd_size,
input_length = max_len) %>%
# add a Convolution1D
layer_conv_1d(filters = 100,
kernel_size = 5,
activation = "relu",
padding = "valid",
strides = 1) %>%
# apply max pooling
layer_max_pooling_1d(pool_size = 5) %>%
## add lstm layer
layer_lstm(units = 256) %>%
# add fully connected layer:
layer_dropout(0.3) %>%
layer_dense(units = 128,
activation = "relu") %>%
layer_dropout(0.3) %>%
layer_dense(units = 64,
activation = "relu") %>%
layer_dropout(0.3) %>%
layer_dense(name = "output",
units = 1,
activation = "tanh")
summary(model_cnn_lstm)
early_stop <- callback_early_stopping(monitor = "val_mae",
min_delta = 0.005, # consider changing globally
patience = 10,
restore_best_weights = TRUE,
verbose = 1)
save_cp <- callback_model_checkpoint("checkpoints/weights.{epoch:02d}-{val_loss:.2f}.hdf5",
monitor = "val_loss",
save_best_only = TRUE,
verbose = 1)
## specify model optimizer, loss and metrics
model_cnn_lstm %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = 'mae'
)
history <- model_cnn_lstm %>%
keras::fit(x_train,
y_train,
epochs = 500,
batch_size = 64,
validation_split = 0.3,
verbose = 2,
callbacks = list(early_stop, save_cp))
plot(history)+
theme_minimal()+
ggtitle("Loss and Accuracy Curves")
## Hyperparameter tuning
## predict on train data
ypred_train_cnn_lstm <- predict(model_cnn_lstm, x_train)
dat_pred_train_cnn_lstm <- bind_cols(y_train, ypred_train_cnn_lstm) %>%
rename(obs = `score`, pred = `...2`) %>%
mutate(obs = as.factor(ifelse(obs > 0, 1,0)),
pred = as.factor(ifelse(pred > 0, 1, 0)))
ConfusionMat <- dat_pred_train_cnn_lstm %>% conf_mat(obs, pred)
# autoplot(ConfusionMat, type = "mosaic")
# autoplot(ConfusionMat, type = "heatmap")
custom_metrics <- metric_set(yardstick::accuracy, sens, spec, ppv, kap, mcc)
metrics_train_cnn_lstm <- custom_metrics(dat_pred_train_cnn_lstm,
truth = obs,
estimate = pred)
print(metrics_train_cnn_lstm)
# testing performance
ypred_test_cnn_lstm <- predict(model_cnn_lstm, x_test)
## join ids and predictions
dat_pred_test_cnn_lstm <- bind_cols(y_test, ypred_test_cnn_lstm) %>%
rename(obs = `score`, pred = `...2`) %>%
mutate(obs = as.factor(ifelse(obs > 0, 1,0)),
pred = as.factor(ifelse(pred > 0, 1, 0)))
ConfusionMat <- dat_pred_test_cnn_lstm %>% conf_mat(obs, pred)
# autoplot(ConfusionMat, type = "mosaic")
# autoplot(ConfusionMat, type = "heatmap")
custom_metrics <- metric_set(yardstick::accuracy, sens, spec, ppv, kap, mcc)
metrics_test_cnn_lstm <- custom_metrics(dat_pred_test_cnn_lstm,
truth = obs,
estimate = pred)
print(metrics_test_cnn_lstm)
model_cnn_lstm <- keras_model_sequential() %>%
# embedding layer
layer_embedding(name = "input",
input_dim = vocab_size + 1,
output_dim = emd_size,
input_length = max_len) %>%
# add a Convolution1D
layer_conv_1d(filters = 100,
kernel_size = 10,
activation = "relu",
padding = "valid",
strides = 1) %>%
# apply max pooling
layer_max_pooling_1d(pool_size = 5) %>%
## add lstm layer
layer_lstm(units = 256) %>%
# add fully connected layer:
layer_dropout(0.3) %>%
layer_dense(units = 128,
activation = "relu") %>%
layer_dropout(0.3) %>%
layer_dense(units = 64,
activation = "relu") %>%
layer_dropout(0.3) %>%
layer_dense(name = "output",
units = 1,
activation = "tanh")
summary(model_cnn_lstm)
early_stop <- callback_early_stopping(monitor = "val_mae",
min_delta = 0.005, # consider changing globally
patience = 10,
restore_best_weights = TRUE,
verbose = 1)
save_cp <- callback_model_checkpoint("checkpoints/weights.{epoch:02d}-{val_loss:.2f}.hdf5",
monitor = "val_loss",
save_best_only = TRUE,
verbose = 1)
## specify model optimizer, loss and metrics
model_cnn_lstm %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = 'mae'
)
history <- model_cnn_lstm %>%
keras::fit(x_train,
y_train,
epochs = 500,
batch_size = 64,
validation_split = 0.3,
verbose = 2,
callbacks = list(early_stop, save_cp))
plot(history)+
theme_minimal()+
ggtitle("Loss and Accuracy Curves")
## Hyperparameter tuning
## predict on train data
ypred_train_cnn_lstm <- predict(model_cnn_lstm, x_train)
dat_pred_train_cnn_lstm <- bind_cols(y_train, ypred_train_cnn_lstm) %>%
rename(obs = `score`, pred = `...2`) %>%
mutate(obs = as.factor(ifelse(obs > 0, 1,0)),
pred = as.factor(ifelse(pred > 0, 1, 0)))
ConfusionMat <- dat_pred_train_cnn_lstm %>% conf_mat(obs, pred)
# autoplot(ConfusionMat, type = "mosaic")
# autoplot(ConfusionMat, type = "heatmap")
custom_metrics <- metric_set(yardstick::accuracy, sens, spec, ppv, kap, mcc)
metrics_train_cnn_lstm <- custom_metrics(dat_pred_train_cnn_lstm,
truth = obs,
estimate = pred)
print(metrics_train_cnn_lstm)
# testing performance
ypred_test_cnn_lstm <- predict(model_cnn_lstm, x_test)
## join ids and predictions
dat_pred_test_cnn_lstm <- bind_cols(y_test, ypred_test_cnn_lstm) %>%
rename(obs = `score`, pred = `...2`) %>%
mutate(obs = as.factor(ifelse(obs > 0, 1,0)),
pred = as.factor(ifelse(pred > 0, 1, 0)))
ConfusionMat <- dat_pred_test_cnn_lstm %>% conf_mat(obs, pred)
# autoplot(ConfusionMat, type = "mosaic")
# autoplot(ConfusionMat, type = "heatmap")
custom_metrics <- metric_set(yardstick::accuracy, sens, spec, ppv, kap, mcc)
metrics_test_cnn_lstm <- custom_metrics(dat_pred_test_cnn_lstm,
truth = obs,
estimate = pred)
print(metrics_test_cnn_lstm)
model_cnn_lstm <- keras_model_sequential() %>%
# embedding layer
layer_embedding(name = "input",
input_dim = vocab_size + 1,
output_dim = emd_size,
input_length = max_len) %>%
# add a Convolution1D
layer_conv_1d(filters = 100,
kernel_size = 10,
activation = "relu",
padding = "valid",
strides = 1) %>%
# apply max pooling
layer_max_pooling_1d(pool_size = 5) %>%
## add lstm layer
layer_lstm(units = 256) %>%
# add fully connected layer:
layer_dropout(0.3) %>%
layer_dense(units = 256,
activation = "relu") %>%
layer_dropout(0.3) %>%
layer_dense(units = 128,
activation = "relu") %>%
layer_dropout(0.3) %>%
layer_dense(units = 64,
activation = "relu") %>%
layer_dropout(0.3) %>%
layer_dense(name = "output",
units = 1,
activation = "tanh")
summary(model_cnn_lstm)
early_stop <- callback_early_stopping(monitor = "val_mae",
min_delta = 0.005, # consider changing globally
patience = 10,
restore_best_weights = TRUE,
verbose = 1)
save_cp <- callback_model_checkpoint("checkpoints/weights.{epoch:02d}-{val_loss:.2f}.hdf5",
monitor = "val_loss",
save_best_only = TRUE,
verbose = 1)
## specify model optimizer, loss and metrics
model_cnn_lstm %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = 'mae'
)
history <- model_cnn_lstm %>%
keras::fit(x_train,
y_train,
epochs = 500,
batch_size = 64,
validation_split = 0.3,
verbose = 2,
callbacks = list(early_stop, save_cp))
plot(history)+
theme_minimal()+
ggtitle("Loss and Accuracy Curves")
## Hyperparameter tuning
## predict on train data
ypred_train_cnn_lstm <- predict(model_cnn_lstm, x_train)
dat_pred_train_cnn_lstm <- bind_cols(y_train, ypred_train_cnn_lstm) %>%
rename(obs = `score`, pred = `...2`) %>%
mutate(obs = as.factor(ifelse(obs > 0, 1,0)),
pred = as.factor(ifelse(pred > 0, 1, 0)))
ConfusionMat <- dat_pred_train_cnn_lstm %>% conf_mat(obs, pred)
# autoplot(ConfusionMat, type = "mosaic")
# autoplot(ConfusionMat, type = "heatmap")
custom_metrics <- metric_set(yardstick::accuracy, sens, spec, ppv, kap, mcc)
metrics_train_cnn_lstm <- custom_metrics(dat_pred_train_cnn_lstm,
truth = obs,
estimate = pred)
print(metrics_train_cnn_lstm)
# testing performance
ypred_test_cnn_lstm <- predict(model_cnn_lstm, x_test)
## join ids and predictions
dat_pred_test_cnn_lstm <- bind_cols(y_test, ypred_test_cnn_lstm) %>%
rename(obs = `score`, pred = `...2`) %>%
mutate(obs = as.factor(ifelse(obs > 0, 1,0)),
pred = as.factor(ifelse(pred > 0, 1, 0)))
ConfusionMat <- dat_pred_test_cnn_lstm %>% conf_mat(obs, pred)
# autoplot(ConfusionMat, type = "mosaic")
# autoplot(ConfusionMat, type = "heatmap")
custom_metrics <- metric_set(yardstick::accuracy, sens, spec, ppv, kap, mcc)
metrics_test_cnn_lstm <- custom_metrics(dat_pred_test_cnn_lstm,
truth = obs,
estimate = pred)
print(metrics_test_cnn_lstm)
model_cnn_lstm <- keras_model_sequential() %>%
# embedding layer
layer_embedding(name = "input",
input_dim = vocab_size + 1,
output_dim = emd_size,
input_length = max_len) %>%
# add a Convolution1D
layer_conv_1d(filters = 100,
kernel_size = 10,
activation = "relu",
padding = "valid",
strides = 1) %>%
# apply max pooling
layer_max_pooling_1d(pool_size = 5) %>%
## add lstm layer
layer_lstm(units = 256) %>%
layer_dropout(0.5) %>%
# add fully connected layer:
layer_dense(units = 128,
activation = "relu") %>%
layer_dropout(0.3) %>%
layer_dense(units = 64,
activation = "relu") %>%
layer_dropout(0.3) %>%
layer_dense(name = "output",
units = 1,
activation = "tanh")
summary(model_cnn_lstm)
early_stop <- callback_early_stopping(monitor = "val_mae",
min_delta = 0.005, # consider changing globally
patience = 10,
restore_best_weights = TRUE,
verbose = 1)
save_cp <- callback_model_checkpoint("checkpoints/weights.{epoch:02d}-{val_loss:.2f}.hdf5",
monitor = "val_loss",
save_best_only = TRUE,
verbose = 1)
## specify model optimizer, loss and metrics
model_cnn_lstm %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = 'mae'
)
history <- model_cnn_lstm %>%
keras::fit(x_train,
y_train,
epochs = 500,
batch_size = 64,
validation_split = 0.3,
verbose = 2,
callbacks = list(early_stop, save_cp))
plot(history)+
theme_minimal()+
ggtitle("Loss and Accuracy Curves")
## Hyperparameter tuning
## predict on train data
ypred_train_cnn_lstm <- predict(model_cnn_lstm, x_train)
dat_pred_train_cnn_lstm <- bind_cols(y_train, ypred_train_cnn_lstm) %>%
rename(obs = `score`, pred = `...2`) %>%
mutate(obs = as.factor(ifelse(obs > 0, 1,0)),
pred = as.factor(ifelse(pred > 0, 1, 0)))
ConfusionMat <- dat_pred_train_cnn_lstm %>% conf_mat(obs, pred)
# autoplot(ConfusionMat, type = "mosaic")
# autoplot(ConfusionMat, type = "heatmap")
custom_metrics <- metric_set(yardstick::accuracy, sens, spec, ppv, kap, mcc)
metrics_train_cnn_lstm <- custom_metrics(dat_pred_train_cnn_lstm,
truth = obs,
estimate = pred)
print(metrics_train_cnn_lstm)
# testing performance
ypred_test_cnn_lstm <- predict(model_cnn_lstm, x_test)
## join ids and predictions
dat_pred_test_cnn_lstm <- bind_cols(y_test, ypred_test_cnn_lstm) %>%
rename(obs = `score`, pred = `...2`) %>%
mutate(obs = as.factor(ifelse(obs > 0, 1,0)),
pred = as.factor(ifelse(pred > 0, 1, 0)))
ConfusionMat <- dat_pred_test_cnn_lstm %>% conf_mat(obs, pred)
# autoplot(ConfusionMat, type = "mosaic")
# autoplot(ConfusionMat, type = "heatmap")
custom_metrics <- metric_set(yardstick::accuracy, sens, spec, ppv, kap, mcc)
metrics_test_cnn_lstm <- custom_metrics(dat_pred_test_cnn_lstm,
truth = obs,
estimate = pred)
print(metrics_test_cnn_lstm)
model_cnn_lstm <- keras_model_sequential() %>%
# embedding layer
layer_embedding(name = "input",
input_dim = vocab_size + 1,
output_dim = emd_size,
input_length = max_len) %>%
# add a Convolution1D
layer_conv_1d(filters = 100,
kernel_size = 10,
activation = "relu",
padding = "valid",
strides = 1) %>%
# apply max pooling
layer_max_pooling_1d(pool_size = 5) %>%
## add lstm layer
layer_lstm(units = 256) %>%
layer_dropout(0.5) %>%
# add fully connected layer:
layer_dense(units = 128,
activation = "relu") %>%
layer_dropout(0.5) %>%
layer_dense(units = 64,
activation = "relu") %>%
layer_dropout(0.3) %>%
layer_dense(name = "output",
units = 1,
activation = "tanh")
summary(model_cnn_lstm)
early_stop <- callback_early_stopping(monitor = "val_mae",
min_delta = 0.005, # consider changing globally
patience = 10,
restore_best_weights = TRUE,
verbose = 1)
save_cp <- callback_model_checkpoint("checkpoints/weights.{epoch:02d}-{val_loss:.2f}.hdf5",
monitor = "val_loss",
save_best_only = TRUE,
verbose = 1)
## specify model optimizer, loss and metrics
model_cnn_lstm %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = 'mae'
)
history <- model_cnn_lstm %>%
keras::fit(x_train,
y_train,
epochs = 500,
batch_size = 64,
validation_split = 0.3,
verbose = 2,
callbacks = list(early_stop, save_cp))
model_cnn_lstm <- keras_model_sequential() %>%
# embedding layer
layer_embedding(name = "input",
input_dim = vocab_size + 1,
output_dim = emd_size,
input_length = max_len) %>%
# add a Convolution1D
layer_conv_1d(filters = 100,
kernel_size = 7,
activation = "relu",
padding = "valid",
strides = 1) %>%
# apply max pooling
layer_max_pooling_1d(pool_size = 5) %>%
## add lstm layer
layer_lstm(units = 256) %>%
layer_dropout(0.5) %>%
# add fully connected layer:
layer_dense(units = 128,
activation = "relu") %>%
layer_dropout(0.3) %>%
layer_dense(units = 64,
activation = "relu") %>%
layer_dropout(0.3) %>%
layer_dense(name = "output",
units = 1,
activation = "tanh")
summary(model_cnn_lstm)
early_stop <- callback_early_stopping(monitor = "val_mae",
min_delta = 0.005, # consider changing globally
patience = 10,
restore_best_weights = TRUE,
verbose = 1)
save_cp <- callback_model_checkpoint("checkpoints/weights.{epoch:02d}-{val_loss:.2f}.hdf5",
monitor = "val_loss",
save_best_only = TRUE,
verbose = 1)
## specify model optimizer, loss and metrics
model_cnn_lstm %>% compile(
optimizer = 'adam',
loss = 'mse',
metrics = 'mae'
)
history <- model_cnn_lstm %>%
keras::fit(x_train,
y_train,
epochs = 500,
batch_size = 64,
validation_split = 0.3,
verbose = 2,
callbacks = list(early_stop, save_cp))
plot(history)+
theme_minimal()+
ggtitle("Loss and Accuracy Curves")
## Hyperparameter tuning
## predict on train data
ypred_train_cnn_lstm <- predict(model_cnn_lstm, x_train)
dat_pred_train_cnn_lstm <- bind_cols(y_train, ypred_train_cnn_lstm) %>%
rename(obs = `score`, pred = `...2`) %>%
mutate(obs = as.factor(ifelse(obs > 0, 1,0)),
pred = as.factor(ifelse(pred > 0, 1, 0)))
ConfusionMat <- dat_pred_train_cnn_lstm %>% conf_mat(obs, pred)
# autoplot(ConfusionMat, type = "mosaic")
# autoplot(ConfusionMat, type = "heatmap")
custom_metrics <- metric_set(yardstick::accuracy, sens, spec, ppv, kap, mcc)
metrics_train_cnn_lstm <- custom_metrics(dat_pred_train_cnn_lstm,
truth = obs,
estimate = pred)
print(metrics_train_cnn_lstm)
# testing performance
ypred_test_cnn_lstm <- predict(model_cnn_lstm, x_test)
## join ids and predictions
dat_pred_test_cnn_lstm <- bind_cols(y_test, ypred_test_cnn_lstm) %>%
rename(obs = `score`, pred = `...2`) %>%
mutate(obs = as.factor(ifelse(obs > 0, 1,0)),
pred = as.factor(ifelse(pred > 0, 1, 0)))
ConfusionMat <- dat_pred_test_cnn_lstm %>% conf_mat(obs, pred)
# autoplot(ConfusionMat, type = "mosaic")
# autoplot(ConfusionMat, type = "heatmap")
custom_metrics <- metric_set(yardstick::accuracy, sens, spec, ppv, kap, mcc)
metrics_test_cnn_lstm <- custom_metrics(dat_pred_test_cnn_lstm,
truth = obs,
estimate = pred)
print(metrics_test_cnn_lstm)
install.packages("tidyr")
install.packages("tidyr")
install.packages("tidyr")
install.packages("tidyr")
install.packages("tidyr")
install.packages("tidyr")
install.packages("tidyr")
shiny::runApp('D:/MyProjects/College Projects/Stats Projects/Shiny/Understanding Alcohol Consumption/shiny_app')
install.packages('tidyr')
install.packages("tidyr")
install.packages("tidyr")
library(tidyr)
install.packages("tidyr")
install.packages("tidyr")
install.packages("tidyr")
install.packages("tidyr")
install.packages("tidyr")
install.packages("tidyr")
install.packages('tidyr')
install.packages("tidyr")
d <- 'D:/MyProjects/College Projects/Stats Projects/Shiny/Understanding Alcohol Consumption')
d <- 'D:/MyProjects/College Projects/Stats Projects/Shiny/Understanding Alcohol Consumption'
setwd(d)
library(rsconnect)
deployApp("D:/MyProjects/College Projects/Stats Projects/Shiny/Understanding Alcohol Consumption/shiny_app", appName="alcohol-consumption")
